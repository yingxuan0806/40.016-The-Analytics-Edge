---
title: "Challenger Notebook"
output:
  pdf_document: default
  html_notebook: default
---
The dataset consists of 144 observations of 5 variables consisting of:  
Flight: name of flight; 
Date (date of flight);  
Field (1 if an Oring fails and 0 otherwise);  
Temp (Temperature in degrees Fahrenheit);   
Pres (Leak check pressure in psi).   
Each flight had 6 orings.
```{r}
orings <- read.csv("Orings.csv")
str(orings)
summary(orings)
```
Let us check the number of orings that have failed out of 6 in each of the flights launched.
```{r}
tapply(orings$Field, orings$Flight, sum)

table(tapply(orings$Field, orings$Flight, sum))
# row: field
# column: flight
# in 16 of the flights, none of the orings failed
# in 5 of the flights, 1 oring failed

```
We plot the failures and temperatures.  
We use jitter plot to randomly perturb points by a small amount to see the points that lie on top of each other better.
```{r}
library(ggplot2)
# plot graphs for orings that has failed
ggplot(orings[orings$Field > 0, ], aes(x = Temp, y = Field)) +  geom_point(na.rm = TRUE)

ggplot(data = orings[orings$Field > 0, ], aes(x = Temp, y = Field)) + geom_point(na.rm = TRUE) + geom_jitter(na.rm = TRUE, width = 0, height = 0.1) + ylim(c(0.5, 1.5))

ggplot(data = orings[orings$Field > 0, ], aes(x = Temp, y = Field)) + geom_point(na.rm = TRUE) + geom_jitter(na.rm = TRUE, width = 2, height = 0) + ylim(c(0.5, 1.5))

# plot graphs of all orings
ggplot(data = orings, aes(x = Temp, y = Field)) + geom_point(na.rm = TRUE) + geom_jitter(na.rm = TRUE, width = 2, height = 0)
```
The plots of temperature with failures only and the plot of temperatures with both failures and non-failures provides different insights. In the former, there are failures across the range with some more at the extremes. In the second case, it is much clearer that there are lesser failures at higher temperatures. It is believed that analysis of plots such as the first one led the managers to conclude that there was not a significant influence of low temperatures.

Fitting a linear regression model

- y: probability of failure/success, 0 <= probability <= 1

```{r}
model1 <- lm(formula = Field ~ Temp + Pres, data = orings)
summary(model1)

# multiple R squared value is quite high
# linear regression perhaps does not explain the data so well
```
Now we use only Temperature
```{r}
model2 <- lm(formula = Field ~ Temp, data = orings)
summary(model2)
# r squared value dropped drastically, so this is not a good model as well

ggplot(orings, aes(Temp, Field)) + geom_jitter(na.rm = T, height = 0, width = 2) + geom_smooth(method = "lm", se = F,na.rm = T, fullrange = T)

```
The result indicates that the linear fit is not particularly convincing with a small value of R-squared, though it does identify that temperature has a significant effect and it is a negative impact.

Fitting a logistic regression model: glm() is a generalized linear model that can be used to fit a logistic regression model by choosing family=binomial.
```{r}
model3 <- glm(formula = Field ~ Temp + Pres, data = orings, family = "binomial")
summary(model3)
```
Now we use only temperature
```{r}
model4 <- glm(formula = Field ~ Temp, data = orings, family = binomial)
summary(model4)
```
```{r}
model3$coefficients
model3$aic
```
Model 3 describes  
$Prob(Fail = 1) = exp(3.95-0.119 Temp + 0.008 Pres)/(1+exp(3.95-0.119Temp+0.008Pres)).$  
Model 3 result indicates that Temp is significant at the 5% level.  
```{r}
model4$coefficients
model4$aic
```

Model 4 has a fit given by   
$P(Fail=1) = exp(6.75-0.1397Temp)/(1+exp(6.75-0.1397Temp))$.  
In terms of AIC, Model 4 is preferred to Model 3, because Model 4 has lower AIC value.
We drop the pressure variable in this example.
Hence, we use model 4 to predict. In this case, failure is only dependent on temperature.

Predictions:
```{r}
# predict value on log(odds) scale
# ie. the exponential function in the numerator (beta.x)

predict(model4, newdata = tail(orings, 1))

# predict value on odds scale
# ie. probability of y = 1
predict(model4, newdata = tail(orings, 1), type = "response")
```
Plots
```{r}
ggplot(data = orings, aes(x = Temp, y = Field)) + geom_jitter(na.rm = T, height = 0, width = 2) 

# Logistic Regression Model Fit
ggplot(data = orings, aes(x = Temp, y = Field)) + geom_jitter(na.rm = T, height = 0, width = 2) + geom_smooth(method = "glm", se = F, na.rm = T, fullrange = T, method.args = list(family = "binomial"))

```
The predicted probability of failure under the model (for a temperature of 31 degree Fahrenheit) is 0.918.

From the logistic regression graph, we can infer that the probability of oring failure is higher at low temperatures relative to high temperatures.
- higher probabilities at low temperatures


Developing a predictive rule (classifier) and tabulating confusion matrices.  
Here we are still evaluatingthis with the training data as we have a small dataset. Typically we will use a test data to check on the results.
```{r}
# type = "response": probability values are returned
Pred <- predict(model4, newdata = orings, type = "response")

# create a table of confusion matrix
# row: prediction label 0 or 1
# column: true label 0 or 1
table(Pred[1:138] > 0.5, orings$Field[1:138])
# Pred > 0.5 means that failure will happen if the threshold is > 0.5, which will predict value of 1
# values show that there will never be a failure if the threshold is set at 0.5
# there were 10 failures that happened but it was predicted as no failure will happen
# hence, model is not appropriate!
# 128 cases were predicted correctly, 10 cases were predicted incorrectly. 
```

```{r}
table(Pred[1:138] > 0.3, orings$Field[1:138])
# threshold of > 0.3 to be a failure
# now the results show that there are lesser wrong predictions on actual failures (ie. predicted as not failure)
```

The ROCR package is useful for visualizing the performance of classifier. The prediction function transforms the data to a standardized format and the performance function is used to do all kinds of prediction evaluations.
```{r}
library(ROCR)
# creates a prediction instance in the required ROCR standardised format
# input values are prediction labels and true labels
ROCRpred <- prediction(predictions = Pred[1:138], labels = orings$Field[1:138])

?performance
# creates the ROC curve
ROCRperf <- performance(prediction.obj = ROCRpred, x.measure = "fpr", measure = "tpr")

plot(ROCRperf)

# indicate the t values across the ROC curve
# cutoffs refer to threshold t values
plot(ROCRperf, colorize = T, print.cutoffs.at = c(0, 0.1, 0.2, 0.3, 0.5, 1), text.adj = c(-0.2, 1.7))

# calculate AUC
as.numeric(performance(prediction.obj = ROCRpred, measure = "auc")@y.values)
```
The AUC for this example is 0.725