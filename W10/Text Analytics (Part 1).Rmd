---
title: "Text Analytics (Part 1)"
author: "Stefano Galelli"
date: "Term 6, 2020"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 3
    number_sections: true
---

First, we prepare the working environment.
```{r}
rm(list=ls()) # Clear the environment
setwd("~/Documents/SUTD/Term 6/TAE/W10")
# setwd("...")  # Setup the working directory
```


# Data Preparation

Load the twitter data.

```{r}
twitter <- read.csv("twitter.csv") 
str(twitter) # Internal structure of the dataframe
```

With the option `stringsAsFactors=FALSE` we make sure that character vectors are NOT converted into factors.

```{r}
twitter <- read.csv("twitter.csv", stringsAsFactors = FALSE)
# twitter <- read.csv("twitter.csv", stringsAsFactors = FALSE, fileEncoding = "latin1") Alternative command for forcing the file encoding
str(twitter)  # 2664 observations of 2 variables
```

A quick look at the dataframe

```{r}
# head(twitter) # First part of the dataframe
# tail(twitter) # Last part of the dataframe
summary(twitter) # Summary of the data
```

A few comments about the data:
- The sentiment takes value 1, 2, 4, and 5, where 1 = very negative, 2 = slightly negative, 4 = slightly positive, and 5 = very positive.
- The tweets were classified by the contributors.
- The data come from [CrowdFlower](https://www.figure-eight.com/data-for-everyone/), a crowdsourcing data management company which allows users to access online workforce of million of people to clean, label, and enrich data.

Let's take a look at the negative and very negative tweets

```{r}
twitter$Neg <- as.factor(twitter$sentiment <= 2)
table(twitter$Neg)
```

So, we have ```r (775/(775+1889))*100```% of tweets with negative sentiment. 


# Pre-processing

To create the DTM, we will use the text mining package `tm`. Note that this is a development package, so there could be some variations based on the system you run (e.g., different R versions or different operating systems).

```{r}
if(!require(tm)){
  install.packages("tm")
  library(tm)
}
```

A corpus represents a collection of documents (tweets, in our case)

```{r}
?Corpus
```

The `VectorSource` helps interpret each element of the `twitter$tweet` object as a document, while the `Corpus` command helps represent it as a corpus.

```{r}
?VectorSource
corpus <- Corpus(VectorSource(twitter$tweet))
# corpus
```

Elements of the corpus.

```{r}
# first tweet
corpus[[1]]
as.character(corpus[[1]])

# final tweet
corpus[[2664]]
as.character(corpus[[2664]])
```

With the function `tm_map`, we apply mappings (transformations) to the corpus. 

```{r}
?tm_map
```

There is a variety of transformations that we can apply

```{r}
getTransformations()
  ```

## Convert text to lower case

The first goal is to convert all text to lower case (since R is case sensitive). Although there is a `tolower` transformation, it is not a part of the standard `tm` transformations (see the output of `getTransformations()` in the previous section). For this reason, we have to convert tolower into a transformation that can handle a corpus object properly. This is done with the help of the function `content_transformer`.

```{r}
# corpus <- tm_map(corpus,content_transformer(tolower))
# corpus <- tm_map(corpus,tolower) # alternative command
```

If the command does not work, try [this trick](https://stackoverflow.com/questions/27756693/fun-error-after-running-tolower-while-making-twitter-wordcloud):

```{r, message=FALSE}
corpus <- tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus <- tm_map(corpus, content_transformer(function(x)    iconv(enc2utf8(x), sub = "bytes")))
corpus <- tm_map(corpus, content_transformer(tolower))
```

Let's now check a couple of documents

```{r}
as.character(corpus[[2664]])
as.character(corpus[[1]])
```

## Remove stopwords

Then, we move on to the stopwords. Here is the list of english stopwords in the `tm` package

```{r}
stopwords("english")
```

Let's then remove the stopwords from the corpus

```{r, message=FALSE}
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
```

We also remove the following words: 'drive', 'driving', 'driver', 'self-driving', 'car', and 'cars'. The reason is that many tweets contain these words, which are probably not going to be a predictor of the polarity of the tweets.

```{r, message=FALSE}
corpus <- tm_map(corpus,removeWords,c("drive","driving","driver","self-driving","car","cars"))
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
```

## Remove punctuation

```{r, message=FALSE}
corpus <- tm_map(corpus,removePunctuation)
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
```

## Stemming

Finally, we stem the words using Porter's stemming algorithm. Note that you may need to load the `SnowballC` package to use this functionality. The package is the R interface to the C libstemmer library that implements Porter's word stemming algorithm.

```{r, message=FALSE}
if(!require(SnowballC)){
  install.packages("SnowballC")
  library(SnowballC)
}
```

Stemming:

```{r, message=FALSE}
corpus <- tm_map(corpus, stemDocument)
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
```

Let's compare the raw information with the post-processed one:

```{r}
twitter$tweet[1]
as.character(corpus[[1]])
```

## Create DTM

We can now create a document-term matrix from the original corpus.

```{r}
?DocumentTermMatrix
dtm <- DocumentTermMatrix(corpus)
dtm
```

We have 2664 documents (rows) and 5649 terms (columns). Out of the `r 2664*5649` terms, 22176 terms are nonzero, while the remaining ones are zero. So, sparsity is close to 100%.

Let's check the first document

```{r}
dtm[1,]
inspect(dtm[1,])
```

The non-zero terms are invest, money, place, print and two. We can do this for any document, such as no. 2664.

```{r}
inspect(dtm[2664,])
```

## Removing sparse terms

An important information we can get is the frequency with which terms appear

```{r}
?findFreqTerms
# With this function, we find term appearing with frequency lower than 50%
findFreqTerms(dtm, lowfreq = 50)
```

We can also check the frequency of specific words

```{r}
dtm[,"accid"]
dtm[,"awesom"]
```

This part of the analysis is aimed at removing terms that do not appear frequently

```{r}
?removeSparseTerms
```

In this specific case, we remove all terms with at least 99.5% empty entries

```{r}
dtm <- removeSparseTerms(dtm, 0.995)
dtm # 
```
 
`dtm` is now a term-document matrix with 2664 documents and 265 terms.


# Preparing the DTM for model learning

We transform the term-document matrix into a matrix and, then, into a dataframe

```{r}
twittersparse <- as.data.frame(as.matrix(dtm))
# str(twittersparse)
# colnames(twittersparse)
```

This helps ensure that columns have valid names. For example, names starting with numbers are modified (e.g., 300k -> X300k).

```{r}
colnames(twittersparse) <- make.names(colnames(twittersparse))
# colnames(twittersparse)
```

Basic visualization with wordcloud

```{r}
# Load the wordcloud package 
if(!require(wordcloud)){
  install.packages("wordcloud")
  library(wordcloud)
}
# Get word counts in decreasing order
word_freqs = sort(colSums(twittersparse), decreasing = TRUE)
# Create data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = unname(word_freqs))

# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
```

Last step, we add the output variable (polarity of tweets) to the `twittersparse` dataframe. This is something we'll need to make predictions.

```{r}
twittersparse$Neg <- twitter$Neg
str(twittersparse)
```


# Train and test a classifier

Let's prepare the data for our modelling exercise:

```{r}
# Load the caTools package and set the seed
if(!require(caTools)){
  install.packages("caTools")
  library(caTools)
}
set.seed(123)
# Create train and test sets (with balanced response)
spl <- sample.split(twittersparse$Neg, SplitRatio = 0.7)
train <- subset(twittersparse, spl == TRUE)
test <- subset(twittersparse, spl == FALSE)
```

## Logistc regression

Train:

```{r}
model1 <- glm(Neg ~ ., data = train, family = binomial)
summary(model1)
```

Let's take a quick look at the sign of some coefficients
- `awesom` = -3.144 (lesser chance of negative sentiments)
- `never`  = 1.454  (higher chance of negative sentiments)

Let's now try to make a prediction on the test data:

```{r}
predict1 <- predict(model1,newdata=test,type="response")
table(predict1>=0.5,test$Neg)
```

The model accuracy is ```r (479+110)/800```. 


# CART

Train: 

```{r}
library(rpart)
library(rpart.plot)

model2 <- rpart(Neg~.,data=train)
summary(model2)
```

Tree visualization:

```{r}
prp(model2,type=4,extra=4) # Here, TRUE indicates Neg = 1, FALSE indicates Neg = 0.
```

Prediction:

```{r}
predict2 <- predict(model2,newdata=test,type="class")
table(predict2,test$Neg)
```

The model accuracy is ```(560+16)/800```.

Let's now try to build a deeper tree:

```{r}
model2a <- rpart(Neg~.,data=train,cp=10^-6) 
prp(model2a,type=4,extra=4) 
printcp(model2a)
# plotcp(model2a)
```

And now we prune it:

```{r}
model2b <- prune(model2a,cp=0.00092251) # To prune, we use the smallest value of xerror
prp(model2b,type=4,extra=4) 
```

Prediction:

```{r}
predict2b <- predict(model2b,newdata=test,type="class")
table(predict2b,test$Neg)
```

The model accuracy is ```(523+49)/800```.

# Random Forests

Train:

```{r}
library(randomForest)
# We use the default parameters to fit the model (500 trees)
model3 <- randomForest(Neg~.,data=train) 
# summary(model3)
model3 
```

Prediction

```{r}
predict3 <- predict(model3,newdata=test,type="class")
table(predict3,test$Neg)
```

The accuracy is ```r (469+93)/800```.








