---
title: "Hitters Notebook"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

The hitters dataset consists of 322 observations of 21 variables with the following information - X (name), AtBat, Hits, HmRun (home runs), Runs, RBI, Walks, Years, CAtBat, CHits, CHmRun, CRuns, CRBI, CWalks, League, Division, PutOuts, Assists, Errors, Salary, New League. Here League, Division and NewLeagues are factor variabes with 2 categories. We drop rows with missing entries and are left with 263 observations.
```{r}
rm(list=ls())
hitter <- read.csv("hitters.csv")
# str(hitter)
hitter <- na.omit(hitter)
# str(hitter)
```
The leaps package in R does subset selection with the regsubsets function. By default, the maximum number of subsets, this function uses is 8. We extend this to do a complete subset selection by changing the default value of nvmax argument in this function. Note that CRBI is in the model with 1 to 6 variables but not in the model with 7 and 8 variables.
```{r}
#install.packages("leaps")
library(leaps)
?regsubsets
hitters <- hitter[, 2:21]
model1 <- regsubsets(Salary ~ ., hitters)
summary(model1)
model2 <- regsubsets(Salary ~ ., hitters, nvmax = 19)
summary(model2)

plot(model2)
```
```{r}
names(summary(model2))
summary(model2)$rsq
plot(summary(model2)$rsq)
plot(summary(model2)$rss)
plot(summary(model2)$adjr2)
which.max(summary(model2)$adjr2)
coef(model2,11)
```
The figures indicate that R-squared increase as the number of variables in the subset increases and likewise the residual sum of squared (sum of squared errors) decreases as the size of the subsets increases. On the other hand the adjusted R-squared increases first and then decreases.

Forward stepwise selection: In this example, the best model identified by the forward stepwise selection is the same as that obtained by the best subset selection. It is also possible to run this algorithm using a backward method where you drop variables one a time rather add. In general, the solutions from these two methods can be different.
```{r}
model3<-regsubsets(Salary~.,data=hitters,nvmax=19,method="forward")
which.max(summary(model3)$adjr2)
coef(model3,11)
summary(model2)$adjr2-summary(model3)$adjr2
plot(summary(model3)$adjr2)


model4<-regsubsets(Salary~.,data=hitters,nvmax=19,method="backward")
which.max(summary(model4)$adjr2)
coef(model4,11)
summary(model4)
```

VALIDATION SET:

Split into training and validation set. 
`model.matrix` creates the **X**-matrix. Then we create a loop to find the coefficients for the best subset of size `i` for `i` running from 1 to 19. We use the coefficients to predict the *Y*-values and compute MSE for the *validation set*


```{r, hide ='results'}
# break the dataset into 2 parts: training and validation set
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(hitters), rep = TRUE)
valset <- (!train) # val set is the complement of train set

# validation set approach
# trains only on the training data
model_valset <- regsubsets(Salary ~ .,data = hitters[train,], nvmax = 19)
summary(model_valset)

# create validation set matrix of X to compute MSE values
valset.mat <- model.matrix(Salary ~., data = hitters[valset,])

# valset.mat

# MSE values code
MSE.val = rep(NA ,19)
for (i in 1:19) {
  coefi = coef(model_valset, id = i)
  pred = valset.mat[,names(coefi)] %*% coefi # matrix multiplication
  MSE.val[i] = mean((hitters$Salary[valset] - pred)^2)
}
```
The best model turns out to be the one with 7 variables.  Notice that if we now look at the best subset of size 7 on the entire data set, it is a bit different.
```{r}
MSE.val

plot(MSE.val)

which.min(MSE.val)

coef(model_valset, 7) #### Coefficients from the entire training set of hitters (without NA values)

coef(model2, 7)          #### Coefficients from the entire data set of hitters (without NA values)
```

Unfortunately `regsubsets` do not have a `prediction` method in-built. Hence write a function for prediction:

```{r}
predict.regsubsets <- function(object, newdata, id,...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form,newdata)
  coefi <- coef(object,id=id)
  xvars <- names(coefi)
  mat[,xvars] %*% coefi
}

pred <- predict.regsubsets(model_valset, hitters[valset,], id = 7)
MSE.valset7 <- mean((hitters$Salary[valset] - pred)^2)

MSE.valset7
MSE.val[7]
```

k-FOLD CROSS VALIDATION:
We now try to choose among the models of different sizes using cross validation.
This approach is somewhat involved, as we must perform best
subset selection within each of the `k` training sets. Despite this, we see that
with its clever subsetting syntax, R makes this job quite easy. First, we
create a vector that allocates each observation to one of k = 10 folds, and
we create a matrix in which we will store the results.


```{r}
k <- 10
set.seed (1)
folds <- sample(1:k, nrow(hitters), replace = TRUE)
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))
folds
cv.errors
```
Now we write a for loop that performs cross-validation. In the `j`-th fold, the
elements of folds that equal `j` are in the test set, and the remainder are in
the training set. We make our predictions for each model size (using our
new `predict.regsubsets` function), compute the test errors on the appropriate subset,
and store them in the appropriate slot in the matrix cv.errors.

```{r}
for (j in 1:k) {
  # best.cv means the best subset
 best.cv <- regsubsets(Salary ~ ., data = hitters[folds !=j,], nvmax = 19)
 for (i in 1:19) {
 pred <- predict.regsubsets(best.cv, hitters[folds == j,], id = i)
 cv.errors[j,i] <- mean((hitters$Salary[folds == j] - pred)^2)
 }
}
```
We see that cross-validation selects an 10-variable model. We now perform
best subset selection on the full data set in order to obtain the 10-variable
model.
```{r}
mean.cv.errors <- apply(cv.errors , 2, mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type = 'b')
which.min(mean.cv.errors)


coef(model2,10)
coef(best.cv,10)
```







LASSO: The generalized linear model with penalized maximum likelihood package glmnet in R implements the LASSO method. To run the `glmnet()` function, we need to pass in the arguments as X (input matrix), y (output vector), rather than the y~X format that we used thus far. The model.matrix() function produces a matrix corresponding to the 19 predictors and the intercept and helps transform qualitative variables into dummy quantitative variables. This is important since `glmnet()` works only with quantitative variabes.
```{r}
#install.packages("glmnet")
library(glmnet)
# lasso
# same as above, define the matrix for multiplication, but this time round its for the entire dataset 
X <- model.matrix(Salary ~., hitters)
y <- hitters$Salary
# str(X)
```
We now choose a range for the lambda parameters and create a training and test set. We then build the LASSO model on this data. The output from the model provides the Df (number of nonzeros), %Dev and Lambda values. The deviance measure is given as 2(loglike_sat - loglike), where loglike_sat is the log-likelihood for the saturated model (a model with a free parameter per observation). Null deviance is defined to be
2(loglike_sat - loglike(NULL)) where the NULL model refers to the intercept model only. The deviance ratio is dev.ratio=1-deviance/nulldev. As lambda decreases, the dev.ratio increases (more importance given to model fit than model complexity).
```{r}
# lasso on train set, do cross-validation on test set
grid <- 10^seq(10, -2, length = 100)
set.seed(1)

# make the training set half the data
train <- sample(1:nrow(X), nrow(X) / 2)
test <- -train
modellasso <- glmnet(X[train,], y[train], lambda = grid)
summary(modellasso)
modellasso
deviance(modellasso)
plot(modellasso, xvar = "lambda", label = TRUE)
# df = number of variables chosen (ie. number of non zero beta coefficients)
# %Dev = how much of the null model deviance was explained by the model
# ie. summation (y_i - y_bar) ^2
# no parameter in the model (only the intercept)
# how much of this is explained by the beta values
# for huge lambdas, betas are all zero
```
We see from the plot that as lambda increases, many of the coefficients get close to zero. We can retrieve these coefficients as follows. Note that the number of non-zero coefficients does not change in a fully  monotonic way, as lambda increases or decreases.
```{r}
# number of non-zero beta coefficients
modellasso$df
#modellasso$beta
#coef(modellasso)
```
Predictions: We start with a prediction for the model fitted with lambda = 100. The test mean squared error for this model is 151898.3. Suppose, we change lambda to 50, we get 144900 and if we change lambda to 200, we get 183422.8. Note that by default if prediction is done at lambda values that are not tried in the fitting algorithm, it uses linear interpolation to make predictions. We can use `exact = T` in the argument to get the exact value by refitting. In addition, you need to then pass also the original training set data to the function. You get a test error of 115096 with the full model while 193253 with a very large value of lambda. Thus choosing lambda appropriately will be important in the quality of the fit. This can be done with cross-validation.
```{r}
# fit on test set to get test MSE error value
modellasso$lambda
# s = lambda value
predictlasso1 <- predict(modellasso, newx = X[test,], s = 100)
mean((predictlasso1 - y[test])^2)

predictlasso2 <- predict(modellasso,newx = X[test,], s = 50)
mean((predictlasso2 - y[test])^2)

predictlasso3 <- predict(modellasso,newx = X[test,], s = 200)
mean((predictlasso3 - y[test])^2)




?predict.glmnet
predictlasso1a <- predict(modellasso, newx = X[test,], s = 100, exact = T, x = X[train,], y = y[train])
mean((predictlasso1a - y[test])^2)

predictlasso2a <- predict(modellasso, newx = X[test,], s = 50, exact = T, x = X[train,], y = y[train])
mean((predictlasso2a - y[test])^2)

predictlasso3a <- predict(modellasso, newx = X[test,], s = 200, exact = T, x = X[train,], y = y[train])
mean((predictlasso3a - y[test])^2)

# s = 0 gives MSE value
predictlasso4 <- predict(modellasso, newx = X[test,], s = 0, exact = T, x = X[train,], y = y[train])
mean((predictlasso4 - y[test])^2)

# s = maximum is the null model
predictlasso5 <- predict(modellasso, newx = X[test,], s = 10^10, exact = T, x = X[train,], y = y[train])
mean((predictlasso5 - y[test])^2)
```




Cross-validation: By default, you perform 10 fold cross validation. Note that glmnet uses randomization in choosing the folds which we should be able to control better by setting the seed to be the same. Default k=10. The optimal value of lambda found from cross validation is 8.461927. You can plot the lambda parameter with the cross-validated mean error (cvm). We see that the best fit from model with the optimal lambda gives a much smaller error on the test set than the model which is based on complete linear regression or the model with only an intercept. We can print out the coefficients to identify that 7 variables are chosen (excluding the intercept).
```{r}
set.seed(2)
?cv.glmnet
cvlasso <- cv.glmnet(X[train,], y[train])

# lambda is chosen automatically here
cvlasso$glmnet.fit

# gives minimum lambda value
cvlasso$lambda.min

# after finding the minimum lambda value, use this lambda to validate on the the test
plot(cvlasso$lambda, cvlasso$cvm)
predictlassocv <- predict(modellasso, s = 8.461927, newx = X[test,])
mean((predictlassocv - y[test])^2)
coef(modellasso, s = 8.461927)
```

